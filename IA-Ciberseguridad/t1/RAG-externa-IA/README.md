# RAG - Retrieval-Augmented Generation

Un sistema que indexa documentos y responde preguntas usando bÃºsqueda vectorial + modelo de lenguaje.

## ğŸ¯ Funcionalidades

- IndexaciÃ³n automÃ¡tica de documentos con embeddings vectoriales.
- BÃºsqueda semÃ¡ntica en tiempo real usando pgvector (Neon).
- GeneraciÃ³n de respuestas fundamentadas en contexto.
- ValidaciÃ³n: si no hay informaciÃ³n relevante, responde honestamente.

## ğŸš€ Inicio rÃ¡pido

### 1. Instalar dependencias
```bash
npm install
```

### 2. Configurar `.env.local`
```env
DATABASE_URL=postgresql://...     # Tu conexiÃ³n a Neon
GOOGLE_API_KEY=...                # Google embeddings
GITHUB_MODELS_TOKEN=...           # GitHub Marketplace token
```

### 3. Preparar BD
```bash
npm run db:generate
npm run db:push
```

### 4. Ingestar documento
```bash
npm run ingest
```

### 5. Iniciar servidor
```bash
npm run dev
```
Abre http://localhost:3000 y pregunta sobre RAG.

## ğŸ“ Estructura

```
src/
  app/api/
    rag/          â†’ Pregunta + respuesta (POST)
    health/       â†’ Verificar modelo (GET)
    debug-rag/    â†’ Ver fragmentos (POST)
  page.tsx        â†’ Chat UI
  lib/ai/rag.ts   â†’ BÃºsqueda vectorial
scripts/ingest.ts â†’ Ingesta de documentos
data/documento.txt â†’ Documento a indexar
```

## ğŸ”Œ Endpoints

**POST `/api/rag`** - Pregunta
```bash
curl -X POST http://localhost:3000/api/rag \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"Â¿QuÃ© es RAG?"}]}'
```

**GET `/api/health`** - Verificar modelo

**POST `/api/debug-rag`** - Ver fragmentos recuperados
```bash
curl -X POST http://localhost:3000/api/debug-rag \
  -H "Content-Type: application/json" \
  -d '{"query":"Â¿QuÃ© es RAG?","k":5}'
```

## ğŸ›  TecnologÃ­as

- Next.js 15 + React 19
- PostgreSQL (Neon) + pgvector
- Google Generative AI (embeddings)
- GitHub Models (generaciÃ³n)
- Drizzle ORM

## ğŸ“ Comportamiento

- **Con contexto**: Responde basÃ¡ndose en fragmentos indexados.
- **Sin contexto**: "No tengo suficiente informaciÃ³n en mi base de conocimientos para responder a esa pregunta."
- **Fallback**: gpt-4o-mini â†’ gpt-4o si el primero no estÃ¡ disponible.

Â¡Listo! ğŸ‰
- Lo divide en chunks de 512 caracteres
- Genera embeddings con Google AI (768 dimensiones)
- Almacena chunks + embeddings en PostgreSQL

## ğŸ“ Estructura del Proyecto

```
.
â”œâ”€â”€ data/
â”‚   â””â”€â”€ documento.txt           # Tu documento para indexar
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ ingest.ts              # Pipeline de ingesta
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”‚   â””â”€â”€ schema.ts       # Esquema Drizzle
â”‚   â”‚   â””â”€â”€ ai/
â”‚   â”‚       â””â”€â”€ rag.ts          # FunciÃ³n de bÃºsqueda
â”‚   â””â”€â”€ app/
â”‚       â””â”€â”€ api/
â”‚           â””â”€â”€ rag/
â”‚               â””â”€â”€ route.ts    # API de chat con RAG
â”œâ”€â”€ drizzle.config.ts           # Config de migraciones
â”œâ”€â”€ tsconfig.json
â””â”€â”€ package.json
```

## ğŸ”„ CÃ³mo Funciona

### Pipeline de Ingesta
1. **Cargar documento** â†’ Lee `data/documento.txt`
2. **Chunking** â†’ Divide en fragmentos de 512 caracteres
3. **Embeddings** â†’ Convierte cada chunk en vector de 768 dims
4. **Almacenamiento** â†’ Guarda en tabla `chunks` de PostgreSQL

### Pipeline de RecuperaciÃ³n
1. **Consulta del usuario** â†’ Se convierte en embedding
2. **BÃºsqueda de similitud** â†’ Busca chunks similares en BD
3. **AumentaciÃ³n del prompt** â†’ AÃ±ade contexto al LLM
4. **GeneraciÃ³n** â†’ Google Gemini responde basado en contexto

## ğŸ§ª Pruebas

Crea un archivo `contrib/test.http` para probar con REST Client:

```http
### Pregunta con contexto
POST http://localhost:3000/api/rag
Content-Type: application/json

{
  "messages": [
    {
      "role": "user",
      "content": "Â¿QuÃ© es el Vercel AI SDK?"
    }
  ]
}

### Pregunta sin contexto
POST http://localhost:3000/api/rag
Content-Type: application/json

{
  "messages": [
    {
      "role": "user",
      "content": "Â¿CuÃ¡l es la capital de Marte?"
    }
  ]
}
```

## ğŸ“š Recursos

- [Parte 1: TeorÃ­a de RAG](https://aperezl.com/post/rag-dotando-de-memoria-a-tu-agente-parte-1)
- [Parte 2: ImplementaciÃ³n](https://aperezl.com/post/rag-dotando-de-memoria-a-tu-agente-parte-2)
- [Parte 3: RefactoraciÃ³n y UI](https://aperezl.com/post/rag-dotando-de-memoria-a-tu-agente-parte-3)

## ğŸ› ï¸ Troubleshooting

### Error: "DATABASE_URL not defined"
- Verifica que `.env.local` existe y tiene la URL correcta
- AsegÃºrate de que PostgreSQL estÃ¡ accesible

### Error: "pgvector extension not found"
- Ejecuta en psql: `CREATE EXTENSION IF NOT EXISTS vector;`
- Algunos servicios como Neon lo habilitan automÃ¡ticamente

### Error: "GOOGLE_API_KEY not found"
- ObtÃ©n tu API key en https://aistudio.google.com/app/apikey
- CÃ³piala en `.env.local`

## ğŸ“– PrÃ³ximos Pasos

1. Customizar `data/documento.txt` con tu propio contenido
2. Crear una UI de chat con React/Next.js
3. Implementar mÃºltiples documentos
4. AÃ±adir soporte para PDFs y otros formatos

Â¡Disfruta tu asistente de IA con memoria! ğŸš€
